{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a simple shift-invariant classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expriments from the paper \"Making Vision Transformers Truly Shift-Equivariant\".\\\n",
    "https://arxiv.org/abs/2305.16316\\\n",
    "\n",
    "Some questions I had after reading the paper\n",
    "1. Why would making the ViT adaptive improve raw classification accuracy?\n",
    "2. Why do we need to make each module shift invariant? Why can't we just make the tokenization shift-invariant? Doesn't making the tokenization shift invariant mean that the rest of the model will see the same tokens no matter how you shift the input?\n",
    "3. What is the tradeoff between building the shift equivariance into the architecture, and creating a training set that is sufficiently shifted around / noised. Could you get similar with no change to model architecture results by simply training on data that contains a lot more shifts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = FashionMNIST(root='./data', train=True, transform=transforms, download=True)\n",
    "test_data = FashionMNIST(root='./data', train=False, transform=transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very simple classifier that is just tokenization->embedding->MLP for classification\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, patch_size, n_emb, num_classes, hidden_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_emb = n_emb\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.e_proj = nn.Linear(patch_size, n_emb)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, num_classes),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        B, N = x.shape\n",
    "        patches = x.reshape(B, N // self.patch_size, self.patch_size)\n",
    "        tokens = self.e_proj(patches)\n",
    "        tokens = tokens.mean(dim=-1) # avg global pooling\n",
    "        logits = self.head(tokens)\n",
    "        return logits\n",
    "    def loss(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return self.loss_fn(logits, y)\n",
    "    def accuracy(self, x, y):\n",
    "        logits = self(x)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        y_hat = torch.argmax(probs, dim=-1)\n",
    "        return (y_hat == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00, train loss = 1.03559, val_loss = 0.63789, val_acc = 0.77406\n",
      "Epoch 01, train loss = 0.58487, val_loss = 0.57682, val_acc = 0.78494\n",
      "Epoch 02, train loss = 0.52111, val_loss = 0.51925, val_acc = 0.81130\n",
      "Epoch 03, train loss = 0.48312, val_loss = 0.49681, val_acc = 0.82069\n",
      "Epoch 04, train loss = 0.45684, val_loss = 0.47746, val_acc = 0.82318\n",
      "Epoch 05, train loss = 0.43632, val_loss = 0.45448, val_acc = 0.83716\n",
      "Epoch 06, train loss = 0.42048, val_loss = 0.45153, val_acc = 0.83476\n",
      "Epoch 07, train loss = 0.40782, val_loss = 0.43851, val_acc = 0.83976\n",
      "Epoch 08, train loss = 0.39626, val_loss = 0.42246, val_acc = 0.85044\n",
      "Epoch 09, train loss = 0.38539, val_loss = 0.41569, val_acc = 0.85144\n",
      "Epoch 10, train loss = 0.37869, val_loss = 0.41659, val_acc = 0.85184\n",
      "Epoch 11, train loss = 0.36950, val_loss = 0.40852, val_acc = 0.85443\n",
      "Epoch 12, train loss = 0.36433, val_loss = 0.41353, val_acc = 0.85294\n",
      "Epoch 13, train loss = 0.35869, val_loss = 0.39907, val_acc = 0.85803\n",
      "Epoch 14, train loss = 0.35448, val_loss = 0.40273, val_acc = 0.85453\n",
      "Epoch 15, train loss = 0.35009, val_loss = 0.39258, val_acc = 0.85923\n",
      "Epoch 16, train loss = 0.34566, val_loss = 0.40529, val_acc = 0.85553\n",
      "Epoch 17, train loss = 0.34145, val_loss = 0.40463, val_acc = 0.86142\n",
      "Epoch 18, train loss = 0.33854, val_loss = 0.39479, val_acc = 0.85923\n",
      "Epoch 19, train loss = 0.33547, val_loss = 0.39124, val_acc = 0.86142\n",
      "Epoch 20, train loss = 0.33265, val_loss = 0.38048, val_acc = 0.86731\n",
      "Epoch 21, train loss = 0.33018, val_loss = 0.39365, val_acc = 0.86192\n",
      "Epoch 22, train loss = 0.32692, val_loss = 0.38781, val_acc = 0.86312\n",
      "Epoch 23, train loss = 0.32519, val_loss = 0.38855, val_acc = 0.86472\n",
      "Epoch 24, train loss = 0.32208, val_loss = 0.38776, val_acc = 0.86352\n",
      "Epoch 25, train loss = 0.31987, val_loss = 0.38521, val_acc = 0.86392\n",
      "Epoch 26, train loss = 0.31889, val_loss = 0.38101, val_acc = 0.86552\n",
      "Epoch 27, train loss = 0.31600, val_loss = 0.37507, val_acc = 0.86571\n",
      "Epoch 28, train loss = 0.31357, val_loss = 0.38134, val_acc = 0.86562\n",
      "Epoch 29, train loss = 0.31215, val_loss = 0.37496, val_acc = 0.86901\n",
      "Epoch 30, train loss = 0.31078, val_loss = 0.39376, val_acc = 0.86542\n",
      "Epoch 31, train loss = 0.30804, val_loss = 0.40214, val_acc = 0.85992\n",
      "Epoch 32, train loss = 0.30629, val_loss = 0.37235, val_acc = 0.87041\n",
      "Epoch 33, train loss = 0.30472, val_loss = 0.37455, val_acc = 0.86611\n",
      "Epoch 34, train loss = 0.30319, val_loss = 0.38124, val_acc = 0.86611\n",
      "Epoch 35, train loss = 0.30110, val_loss = 0.37301, val_acc = 0.86881\n",
      "Epoch 36, train loss = 0.30013, val_loss = 0.37599, val_acc = 0.86821\n",
      "Epoch 37, train loss = 0.29754, val_loss = 0.38977, val_acc = 0.86482\n",
      "Epoch 38, train loss = 0.29742, val_loss = 0.37728, val_acc = 0.87031\n",
      "Epoch 39, train loss = 0.29462, val_loss = 0.36720, val_acc = 0.87081\n",
      "Epoch 40, train loss = 0.29360, val_loss = 0.37651, val_acc = 0.87091\n",
      "Epoch 41, train loss = 0.29251, val_loss = 0.37147, val_acc = 0.87161\n",
      "Epoch 42, train loss = 0.29093, val_loss = 0.37005, val_acc = 0.87181\n",
      "Epoch 43, train loss = 0.28911, val_loss = 0.37591, val_acc = 0.86751\n",
      "Epoch 44, train loss = 0.28742, val_loss = 0.37505, val_acc = 0.87011\n",
      "Epoch 45, train loss = 0.28651, val_loss = 0.37341, val_acc = 0.87260\n",
      "Epoch 46, train loss = 0.28579, val_loss = 0.37436, val_acc = 0.86901\n",
      "Epoch 47, train loss = 0.28414, val_loss = 0.37640, val_acc = 0.87250\n",
      "Epoch 48, train loss = 0.28358, val_loss = 0.36627, val_acc = 0.87440\n",
      "Epoch 49, train loss = 0.28125, val_loss = 0.37756, val_acc = 0.86741\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "patch_size = 4\n",
    "model = Classifier(\n",
    "    patch_size=patch_size,\n",
    "    n_emb=32,\n",
    "    num_classes=10,\n",
    "    hidden_size=(14*14)//patch_size\n",
    ")\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for input, label in train_dataloader:\n",
    "        # print(input.shape, label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(input, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        loss = model.loss(input, label)\n",
    "        val_acc += model.accuracy(input, label)\n",
    "        running_loss += loss.item()\n",
    "    val_accuracy.append(val_acc / len(test_dataloader))\n",
    "    avg_val_loss = running_loss / len(test_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch:02d}, train loss = {avg_train_loss:.5f}, val_loss = {avg_val_loss:.5f}, val_acc = {val_accuracy[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very simple classifier that is just tokenization->embedding->MLP for classification\n",
    "class AdaptiveClassifier(nn.Module):\n",
    "    def __init__(self, patch_size, n_emb, num_classes, hidden_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_emb = n_emb\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.e_proj = nn.Linear(patch_size, n_emb)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, num_classes),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        N = 14*14\n",
    "        self.roll_idxs = (torch.arange(N)[:, None] - torch.arange(N)) % N\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        B, N = x.shape\n",
    "        # F = l2 norm, calculate m_star\n",
    "        with torch.no_grad():\n",
    "            # For each batch, try each roll from 0..N-1            \n",
    "            shifts = torch.stack([x[i, self.roll_idxs] for i in range(B)]).mT\n",
    "            patches = shifts.view(B, N, N // self.patch_size, self.patch_size)\n",
    "            tokens = self.e_proj(patches)\n",
    "            norms = tokens.norm(dim=(2, 3))\n",
    "            m_star = norms.argmax(dim=1)\n",
    "            # print(norms.shape, m_star.shape)\n",
    "        for i in range(B):\n",
    "            x[i] = torch.roll(x[i], shifts=m_star[i].item())\n",
    "        patches = x.reshape(B, N // self.patch_size, self.patch_size)\n",
    "        tokens = self.e_proj(patches)\n",
    "        tokens = tokens.mean(dim=-1) # avg global pooling\n",
    "        logits = self.head(tokens)\n",
    "        return logits\n",
    "    def loss(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return self.loss_fn(logits, y)\n",
    "    def accuracy(self, x, y):\n",
    "        logits = self(x)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        y_hat = torch.argmax(probs, dim=-1)\n",
    "        return (y_hat == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00, train loss = 2.07253, val_loss = 1.41390, val_acc = 0.46396\n",
      "Epoch 01, train loss = 1.17637, val_loss = 1.07518, val_acc = 0.59165\n",
      "Epoch 02, train loss = 1.02649, val_loss = 1.00768, val_acc = 0.62720\n",
      "Epoch 03, train loss = 0.95728, val_loss = 0.95449, val_acc = 0.63269\n",
      "Epoch 04, train loss = 0.91092, val_loss = 0.89455, val_acc = 0.66014\n",
      "Epoch 05, train loss = 0.87358, val_loss = 0.87685, val_acc = 0.67332\n",
      "Epoch 06, train loss = 0.84198, val_loss = 0.86405, val_acc = 0.68311\n",
      "Epoch 07, train loss = 0.81698, val_loss = 0.80702, val_acc = 0.69319\n",
      "Epoch 08, train loss = 0.79864, val_loss = 0.81607, val_acc = 0.69239\n",
      "Epoch 09, train loss = 0.78502, val_loss = 0.79122, val_acc = 0.70707\n",
      "Epoch 10, train loss = 0.76837, val_loss = 0.78953, val_acc = 0.70966\n",
      "Epoch 11, train loss = 0.76013, val_loss = 0.78583, val_acc = 0.71306\n",
      "Epoch 12, train loss = 0.74701, val_loss = 0.78499, val_acc = 0.71446\n",
      "Epoch 13, train loss = 0.73917, val_loss = 0.75675, val_acc = 0.72943\n",
      "Epoch 14, train loss = 0.72961, val_loss = 0.75371, val_acc = 0.72244\n",
      "Epoch 15, train loss = 0.72329, val_loss = 0.73593, val_acc = 0.73293\n",
      "Epoch 16, train loss = 0.71555, val_loss = 0.73834, val_acc = 0.72674\n",
      "Epoch 17, train loss = 0.70948, val_loss = 0.74249, val_acc = 0.73283\n",
      "Epoch 18, train loss = 0.70145, val_loss = 0.71930, val_acc = 0.73283\n",
      "Epoch 19, train loss = 0.69606, val_loss = 0.72540, val_acc = 0.73303\n",
      "Epoch 20, train loss = 0.68885, val_loss = 0.71076, val_acc = 0.73642\n",
      "Epoch 21, train loss = 0.68431, val_loss = 0.70567, val_acc = 0.74032\n",
      "Epoch 22, train loss = 0.68139, val_loss = 0.70306, val_acc = 0.73822\n",
      "Epoch 23, train loss = 0.67793, val_loss = 0.70669, val_acc = 0.73932\n",
      "Epoch 24, train loss = 0.67276, val_loss = 0.69373, val_acc = 0.74631\n",
      "Epoch 25, train loss = 0.67256, val_loss = 0.69071, val_acc = 0.75120\n",
      "Epoch 26, train loss = 0.66740, val_loss = 0.69221, val_acc = 0.74840\n",
      "Epoch 27, train loss = 0.66458, val_loss = 0.69759, val_acc = 0.74690\n",
      "Epoch 28, train loss = 0.66075, val_loss = 0.67425, val_acc = 0.75469\n",
      "Epoch 29, train loss = 0.65970, val_loss = 0.69040, val_acc = 0.74621\n",
      "Epoch 30, train loss = 0.65458, val_loss = 0.67820, val_acc = 0.75290\n",
      "Epoch 31, train loss = 0.65376, val_loss = 0.67183, val_acc = 0.75649\n",
      "Epoch 32, train loss = 0.64790, val_loss = 0.66630, val_acc = 0.75899\n",
      "Epoch 33, train loss = 0.64896, val_loss = 0.66322, val_acc = 0.75699\n",
      "Epoch 34, train loss = 0.64873, val_loss = 0.67158, val_acc = 0.75399\n",
      "Epoch 35, train loss = 0.64676, val_loss = 0.65218, val_acc = 0.76567\n",
      "Epoch 36, train loss = 0.64012, val_loss = 0.68431, val_acc = 0.74920\n",
      "Epoch 37, train loss = 0.63803, val_loss = 0.66906, val_acc = 0.75799\n",
      "Epoch 38, train loss = 0.63706, val_loss = 0.65771, val_acc = 0.76298\n",
      "Epoch 39, train loss = 0.63580, val_loss = 0.65289, val_acc = 0.76078\n",
      "Epoch 40, train loss = 0.63417, val_loss = 0.66623, val_acc = 0.75699\n",
      "Epoch 41, train loss = 0.63515, val_loss = 0.66868, val_acc = 0.76358\n",
      "Epoch 42, train loss = 0.62904, val_loss = 0.66976, val_acc = 0.75519\n",
      "Epoch 43, train loss = 0.63077, val_loss = 0.65901, val_acc = 0.75988\n",
      "Epoch 44, train loss = 0.62831, val_loss = 0.65184, val_acc = 0.76148\n",
      "Epoch 45, train loss = 0.63081, val_loss = 0.65200, val_acc = 0.76448\n",
      "Epoch 46, train loss = 0.62995, val_loss = 0.65877, val_acc = 0.76278\n",
      "Epoch 47, train loss = 0.62743, val_loss = 0.64555, val_acc = 0.76607\n",
      "Epoch 48, train loss = 0.62641, val_loss = 0.65239, val_acc = 0.76238\n",
      "Epoch 49, train loss = 0.62070, val_loss = 0.65050, val_acc = 0.76268\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "patch_size = 4\n",
    "adaptive_model = AdaptiveClassifier(\n",
    "    patch_size=patch_size,\n",
    "    n_emb=32,\n",
    "    num_classes=10,\n",
    "    hidden_size=(14*14)//patch_size\n",
    ")\n",
    "optimizer = SGD(adaptive_model.parameters(), lr=0.1)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "for epoch in range(50):\n",
    "    adaptive_model.train()\n",
    "    running_loss = 0\n",
    "    for input, label in train_dataloader:\n",
    "        # print(input.shape, label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = adaptive_model.loss(input, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    adaptive_model.eval()\n",
    "    running_loss = 0\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        loss = adaptive_model.loss(input, label)\n",
    "        val_acc += adaptive_model.accuracy(input, label)\n",
    "        running_loss += loss.item()\n",
    "    val_accuracy.append(val_acc / len(test_dataloader))\n",
    "    avg_val_loss = running_loss / len(test_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch:02d}, train loss = {avg_train_loss:.5f}, val_loss = {avg_val_loss:.5f}, val_acc = {val_accuracy[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "val accuracy:  tensor(0.8674)\n",
      "shifted val accuracy: tensor(0.2254)\n",
      "---\n",
      "Classifier w/ adaptive tokenization:\n",
      "val accuracy:  tensor(0.7627)\n",
      "shifted val accuracy: tensor(0.7549)\n"
     ]
    }
   ],
   "source": [
    "def get_val_acc(model, shift=False):\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        B = input.shape[0]\n",
    "        if shift:\n",
    "            offset = torch.randint(1, 5, (1,))\n",
    "            input = torch.roll(input.view(B, -1), shifts=offset.item(), dims=1).view(input.shape)\n",
    "        val_acc += model.accuracy(input, label)\n",
    "    return val_acc / len(test_dataloader)\n",
    "\n",
    "print(\"Classifier:\")\n",
    "print(\"val accuracy: \", get_val_acc(model))\n",
    "print(\"shifted val accuracy:\", get_val_acc(model, shift=True))\n",
    "print(\"---\")\n",
    "print(\"Classifier w/ adaptive tokenization:\")\n",
    "print(\"val accuracy: \", get_val_acc(adaptive_model))\n",
    "print(\"shifted val accuracy:\", get_val_acc(adaptive_model, shift=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive tokenization makes the classifier robust as expected.\\\n",
    "Now I want to try training a non-adaptive classifier with augmented input data to see if it can become similarly robust without architecture changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new training dataset with every shift (0-5) for every example\n",
    "\n",
    "new_X, new_y = [], []\n",
    "for input, label in train_dataloader:\n",
    "    B, C, H, W = input.shape\n",
    "    input = input.view(B, -1)\n",
    "    for r in range(5):\n",
    "        new_X.append(torch.roll(input, shifts=r))\n",
    "        new_y.append(label)\n",
    "new_X = torch.vstack(new_X).view(-1, 14, 14)\n",
    "new_y = torch.stack(new_y).view(-1)\n",
    "\n",
    "class shiftedTrainingData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = new_X\n",
    "        self.targets = new_y\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00, train loss = 0.81551, val_loss = 0.72434, val_acc = 0.74651\n",
      "Epoch 01, train loss = 0.53689, val_loss = 0.64040, val_acc = 0.76997\n",
      "Epoch 02, train loss = 0.48890, val_loss = 0.63119, val_acc = 0.77077\n",
      "Epoch 03, train loss = 0.46213, val_loss = 0.62897, val_acc = 0.76897\n",
      "Epoch 04, train loss = 0.44527, val_loss = 0.57735, val_acc = 0.79363\n",
      "Epoch 05, train loss = 0.43059, val_loss = 0.57648, val_acc = 0.79553\n",
      "Epoch 06, train loss = 0.42064, val_loss = 0.58249, val_acc = 0.78325\n",
      "Epoch 07, train loss = 0.41286, val_loss = 0.56082, val_acc = 0.79603\n",
      "Epoch 08, train loss = 0.40455, val_loss = 0.53400, val_acc = 0.81160\n",
      "Epoch 09, train loss = 0.40016, val_loss = 0.54917, val_acc = 0.80681\n"
     ]
    }
   ],
   "source": [
    "# training with augmented data\n",
    "augmented_train_data = shiftedTrainingData()\n",
    "train_dataloader = DataLoader(augmented_train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "patch_size = 4\n",
    "model2 = Classifier(\n",
    "    patch_size=patch_size,\n",
    "    n_emb=32,\n",
    "    num_classes=10,\n",
    "    hidden_size=(14*14)//patch_size\n",
    ")\n",
    "optimizer = SGD(model2.parameters(), lr=0.1)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "for epoch in range(10):\n",
    "    model2.train()\n",
    "    running_loss = 0\n",
    "    for input, label in train_dataloader:\n",
    "        # random shifting\n",
    "        B = input.shape[0]\n",
    "        offsets = torch.randint(0, 5, (B,))\n",
    "        for i in range(len(input)):\n",
    "            input[i] = torch.roll(input[i], shifts=offsets[i].item())\n",
    "        optimizer.zero_grad()\n",
    "        loss = model2.loss(input, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    model2.eval()\n",
    "    running_loss = 0\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        loss = model2.loss(input, label)\n",
    "        val_acc += model2.accuracy(input, label)\n",
    "        running_loss += loss.item()\n",
    "    val_accuracy.append(val_acc / len(test_dataloader))\n",
    "    avg_val_loss = running_loss / len(test_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch:02d}, train loss = {avg_train_loss:.5f}, val_loss = {avg_val_loss:.5f}, val_acc = {val_accuracy[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained w/ augmentation:\n",
      "val accuracy:  tensor(0.8068)\n",
      "shifted val accuracy: tensor(0.8197)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier trained w/ augmentation:\")\n",
    "print(\"val accuracy: \", get_val_acc(model2))\n",
    "print(\"shifted val accuracy:\", get_val_acc(model2, shift=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training augmentation does make the unmodified model more robust.\n",
    "\n",
    "#### Some Final Notes\n",
    "- Adaptive tokenization is robust to shifting without any training as expected.\n",
    "- Data augmentation can make the unmodified architecture robust to shifting.\n",
    "- The adaptive model was not able to achieve as low of a training loss as the base model.\n",
    "- The adaptive model also trained quite slow, could have been just been my implementation though (need to look into more tensor tricks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
