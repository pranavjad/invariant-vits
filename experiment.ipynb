{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a simple shift-invariant classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = FashionMNIST(root='./data', train=True, transform=transforms, download=True)\n",
    "test_data = FashionMNIST(root='./data', train=False, transform=transforms, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very simple classifier that is just tokenization->embedding->MLP for classification\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, patch_size, n_emb, num_classes, hidden_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_emb = n_emb\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.e_proj = nn.Linear(patch_size, n_emb)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, num_classes),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        B, N = x.shape\n",
    "        patches = x.reshape(B, N // self.patch_size, self.patch_size)\n",
    "        tokens = self.e_proj(patches)\n",
    "        tokens = tokens.mean(dim=-1) # avg global pooling\n",
    "        logits = self.head(tokens)\n",
    "        return logits\n",
    "    def loss(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return self.loss_fn(logits, y)\n",
    "    def accuracy(self, x, y):\n",
    "        logits = self(x)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        y_hat = torch.argmax(probs, dim=-1)\n",
    "        return (y_hat == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00, train loss = 1.25036, val_loss = 0.66491, val_acc = 0.75399\n",
      "Epoch 01, train loss = 0.58810, val_loss = 0.56244, val_acc = 0.79503\n",
      "Epoch 02, train loss = 0.51601, val_loss = 0.52691, val_acc = 0.80351\n",
      "Epoch 03, train loss = 0.47678, val_loss = 0.48850, val_acc = 0.82009\n",
      "Epoch 04, train loss = 0.44947, val_loss = 0.47636, val_acc = 0.82788\n",
      "Epoch 05, train loss = 0.42977, val_loss = 0.45234, val_acc = 0.83536\n",
      "Epoch 06, train loss = 0.41401, val_loss = 0.44127, val_acc = 0.83946\n",
      "Epoch 07, train loss = 0.40164, val_loss = 0.43346, val_acc = 0.84425\n",
      "Epoch 08, train loss = 0.39131, val_loss = 0.42336, val_acc = 0.84675\n",
      "Epoch 09, train loss = 0.38188, val_loss = 0.42639, val_acc = 0.84794\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "patch_size = 4\n",
    "model = Classifier(\n",
    "    patch_size=patch_size,\n",
    "    n_emb=32,\n",
    "    num_classes=10,\n",
    "    hidden_size=(14*14)//patch_size\n",
    ")\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for input, label in train_dataloader:\n",
    "        # print(input.shape, label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(input, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        loss = model.loss(input, label)\n",
    "        val_acc += model.accuracy(input, label)\n",
    "        running_loss += loss.item()\n",
    "    val_accuracy.append(val_acc / len(test_dataloader))\n",
    "    avg_val_loss = running_loss / len(test_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch:02d}, train loss = {avg_train_loss:.5f}, val_loss = {avg_val_loss:.5f}, val_acc = {val_accuracy[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8479)\n",
      "tensor(0.2812)\n"
     ]
    }
   ],
   "source": [
    "def get_val_acc(model, shift=False):\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        B = input.shape[0]\n",
    "        if shift:\n",
    "            offset = torch.randint(1, 5, (1,))\n",
    "            input = torch.roll(input.view(B, -1), shifts=offset.item(), dims=1).view(input.shape)\n",
    "        val_acc += model.accuracy(input, label)\n",
    "    return val_acc / len(test_dataloader)\n",
    "\n",
    "print(\"val accuracy: \", get_val_acc(model))\n",
    "print(\"randomly shifted val accuracy:\", get_val_acc(model, shift=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very simple classifier that is just tokenization->embedding->MLP for classification\n",
    "# class AdaptiveClassifier(nn.Module):\n",
    "#     def __init__(self, patch_size, n_emb, num_classes, hidden_size):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.n_emb = n_emb\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.e_proj = nn.Linear(patch_size, n_emb)\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Linear(hidden_size, hidden_size * 4),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(hidden_size * 4, num_classes),\n",
    "#         )\n",
    "#         self.loss_fn = nn.CrossEntropyLoss()\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         B, N = x.shape\n",
    "#         # F = l2 norm, calculate m_star\n",
    "#         with torch.no_grad():\n",
    "#             # For each batch, try each roll from 0..N-1            \n",
    "#             norms = []\n",
    "#             for m in range(N):\n",
    "#                 xm = torch.roll(x, shifts=m, dims=1)\n",
    "#                 patches = xm.reshape(B, N // self.patch_size, self.patch_size)\n",
    "#                 tokens = self.e_proj(patches)\n",
    "#                 norms.append(tokens.norm(dim=(1, 2)))\n",
    "#             norms = torch.stack(norms)\n",
    "#             m_star = norms.argmax(dim=0)\n",
    "#             # print(m_star.shape)\n",
    "#         for i in range(B):\n",
    "#             x[i] = torch.roll(x[i], shifts=m_star[i].item())\n",
    "#         patches = x.reshape(B, N // self.patch_size, self.patch_size)\n",
    "#         tokens = self.e_proj(patches)\n",
    "#         tokens = tokens.mean(dim=-1) # avg global pooling\n",
    "#         logits = self.head(tokens)\n",
    "#         return logits\n",
    "#     def loss(self, x, y):\n",
    "#         logits = self.forward(x)\n",
    "#         return self.loss_fn(logits, y)\n",
    "#     def accuracy(self, x, y):\n",
    "#         logits = self(x)\n",
    "#         probs = torch.softmax(logits, dim=-1)\n",
    "#         y_hat = torch.argmax(probs, dim=-1)\n",
    "#         return (y_hat == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very simple classifier that is just tokenization->embedding->MLP for classification\n",
    "class AdaptiveClassifier(nn.Module):\n",
    "    def __init__(self, patch_size, n_emb, num_classes, hidden_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_emb = n_emb\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.e_proj = nn.Linear(patch_size, n_emb)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, num_classes),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        B, N = x.shape\n",
    "        # F = l2 norm, calculate m_star\n",
    "        with torch.no_grad():\n",
    "            # For each batch, try each roll from 0..N-1            \n",
    "            roll_idxs = (torch.arange(N)[:, None] - torch.arange(N)) % N\n",
    "            shifts = torch.stack([x[i, roll_idxs] for i in range(B)]).mT\n",
    "            patches = shifts.view(B, N, N // self.patch_size, self.patch_size)\n",
    "            tokens = self.e_proj(patches)\n",
    "            norms = tokens.norm(dim=(2, 3))\n",
    "            m_star = norms.argmax(dim=1)\n",
    "            # print(norms.shape, m_star.shape)\n",
    "        for i in range(B):\n",
    "            x[i] = torch.roll(x[i], shifts=m_star[i].item())\n",
    "        patches = x.reshape(B, N // self.patch_size, self.patch_size)\n",
    "        tokens = self.e_proj(patches)\n",
    "        tokens = tokens.mean(dim=-1) # avg global pooling\n",
    "        logits = self.head(tokens)\n",
    "        return logits\n",
    "    def loss(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return self.loss_fn(logits, y)\n",
    "    def accuracy(self, x, y):\n",
    "        logits = self(x)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        y_hat = torch.argmax(probs, dim=-1)\n",
    "        return (y_hat == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00, train loss = 1.76255, val_loss = 1.20937, val_acc = 0.54203\n",
      "Epoch 01, train loss = 1.11391, val_loss = 1.07206, val_acc = 0.59665\n",
      "Epoch 02, train loss = 0.99718, val_loss = 0.97009, val_acc = 0.62909\n",
      "Epoch 03, train loss = 0.93370, val_loss = 0.91120, val_acc = 0.66104\n",
      "Epoch 04, train loss = 0.89270, val_loss = 0.87649, val_acc = 0.66883\n",
      "Epoch 05, train loss = 0.85919, val_loss = 0.85035, val_acc = 0.67712\n",
      "Epoch 06, train loss = 0.83117, val_loss = 0.83843, val_acc = 0.68620\n",
      "Epoch 07, train loss = 0.80866, val_loss = 0.84389, val_acc = 0.68061\n",
      "Epoch 08, train loss = 0.79503, val_loss = 0.79964, val_acc = 0.70088\n",
      "Epoch 09, train loss = 0.78123, val_loss = 0.81200, val_acc = 0.70288\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "patch_size = 4\n",
    "adaptive_model = AdaptiveClassifier(\n",
    "    patch_size=patch_size,\n",
    "    n_emb=32,\n",
    "    num_classes=10,\n",
    "    hidden_size=(14*14)//patch_size\n",
    ")\n",
    "optimizer = SGD(adaptive_model.parameters(), lr=0.1)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracy = []\n",
    "for epoch in range(10):\n",
    "    adaptive_model.train()\n",
    "    running_loss = 0\n",
    "    for input, label in train_dataloader:\n",
    "        # print(input.shape, label.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = adaptive_model.loss(input, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    adaptive_model.eval()\n",
    "    running_loss = 0\n",
    "    val_acc = 0\n",
    "    for input, label in test_dataloader:\n",
    "        loss = adaptive_model.loss(input, label)\n",
    "        val_acc += adaptive_model.accuracy(input, label)\n",
    "        running_loss += loss.item()\n",
    "    val_accuracy.append(val_acc / len(test_dataloader))\n",
    "    avg_val_loss = running_loss / len(test_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch:02d}, train loss = {avg_train_loss:.5f}, val_loss = {avg_val_loss:.5f}, val_acc = {val_accuracy[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7029)\n",
      "tensor(0.6992)\n"
     ]
    }
   ],
   "source": [
    "print(get_val_acc(adaptive_model))\n",
    "print(get_val_acc(adaptive_model, shift=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.1601), tensor(4.5330), tensor(7.0751), tensor(7.1127))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5, 5)\n",
    "a1 = torch.roll(a, 2)\n",
    "b = torch.randn(5, 5)\n",
    "c = a @ b\n",
    "c1 = a1 @ b\n",
    "a.norm(), b.norm(), c.norm(), c1.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3131],\n",
      "         [-0.1254],\n",
      "         [-0.4524],\n",
      "         [ 0.2357]],\n",
      "\n",
      "        [[ 1.1113],\n",
      "         [-0.3757],\n",
      "         [ 0.0120],\n",
      "         [ 0.2193]]])\n",
      "tensor([[[ 0.3131,  0.3131,  0.3131,  0.3131],\n",
      "         [-0.1254, -0.1254, -0.1254, -0.1254],\n",
      "         [-0.4524, -0.4524, -0.4524, -0.4524],\n",
      "         [ 0.2357,  0.2357,  0.2357,  0.2357]],\n",
      "\n",
      "        [[ 1.1113,  1.1113,  1.1113,  1.1113],\n",
      "         [-0.3757, -0.3757, -0.3757, -0.3757],\n",
      "         [ 0.0120,  0.0120,  0.0120,  0.0120],\n",
      "         [ 0.2193,  0.2193,  0.2193,  0.2193]]])\n",
      "tensor([[0, 3, 2, 1],\n",
      "        [1, 0, 3, 2],\n",
      "        [2, 1, 0, 3],\n",
      "        [3, 2, 1, 0]])\n",
      "tensor([[-0.6804, -1.2364,  0.0966, -0.4540],\n",
      "        [-0.4540, -0.6804, -1.2364,  0.0966],\n",
      "        [ 0.0966, -0.4540, -0.6804, -1.2364],\n",
      "        [-1.2364,  0.0966, -0.4540, -0.6804]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 4, 1)\n",
    "print(a)\n",
    "print(a.expand(2, 4, 4))\n",
    "rolls = torch.arange(4)\n",
    "idxs = (rolls[:, None] - rolls) % 4\n",
    "print(idxs)\n",
    "print(torch.randn(4)[idxs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4199, -0.8493, -0.4666,  0.3999],\n",
      "        [-1.0931, -1.1288, -0.6471,  0.9726],\n",
      "        [-1.3034, -1.3349,  0.2783,  0.7967],\n",
      "        [-0.0505,  0.2216,  0.5465, -0.6069]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4199, -0.8493, -0.4666,  0.3999],\n",
       "         [-1.0931, -1.1288, -0.6471,  0.9726],\n",
       "         [-1.3034, -1.3349,  0.2783,  0.7967],\n",
       "         [-0.0505,  0.2216,  0.5465, -0.6069]],\n",
       "\n",
       "        [[-0.0505,  0.2216,  0.5465, -0.6069],\n",
       "         [ 1.4199, -0.8493, -0.4666,  0.3999],\n",
       "         [-1.0931, -1.1288, -0.6471,  0.9726],\n",
       "         [-1.3034, -1.3349,  0.2783,  0.7967]],\n",
       "\n",
       "        [[-1.3034, -1.3349,  0.2783,  0.7967],\n",
       "         [-0.0505,  0.2216,  0.5465, -0.6069],\n",
       "         [ 1.4199, -0.8493, -0.4666,  0.3999],\n",
       "         [-1.0931, -1.1288, -0.6471,  0.9726]],\n",
       "\n",
       "        [[-1.0931, -1.1288, -0.6471,  0.9726],\n",
       "         [-1.3034, -1.3349,  0.2783,  0.7967],\n",
       "         [-0.0505,  0.2216,  0.5465, -0.6069],\n",
       "         [ 1.4199, -0.8493, -0.4666,  0.3999]]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "a[idxs].transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8327,  0.8327],\n",
       "         [ 0.2480,  0.2480],\n",
       "         [ 1.3505,  1.3505],\n",
       "         [ 0.8663,  0.8663]],\n",
       "\n",
       "        [[-0.6404, -0.6404],\n",
       "         [-0.0786, -0.0786],\n",
       "         [-2.1338, -2.1338],\n",
       "         [ 0.1364,  0.1364]]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 4, 1).repeat((1, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
